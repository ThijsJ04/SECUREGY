# SECUREGY - Assessment Framework

[![GitHub license](https://badgen.net/github/license/Naereen/Strapdown.js)](https://github.com/ThijsJ04/SECUREGY/blob/main/LICENSE)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-no-red.svg)](https://github.com/ThijsJ04/SECUREGY)
[![Python 3.13](https://img.shields.io/badge/python-3.13-blue.svg)](https://www.python.org/downloads/release/python-3133/)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)

SECUREGY presents a novel assessment framework that is designed to evaluate how prompt engineering techniques (PETs) influence both the security and energy efficiency of code generated by large language models (LLMs). SECUREGY was developed to support my bachelor's thesis, which investigated how LLMs can generate code optimized for both energy efficiency and security.

SECUREGY is inspired by two other frameworks: [SALLM](https://github.com/s2e-lab/SALLM) and [GreenCode](https://github.com/tcappendijk/Greencode_framework). Where SALLM is a framework that benchmarks LLMs' abilities to generate secure code systematically [1]. GreenCode is a framework that benchmarks the influence of prompt modification on the energy consumption of the code generated by LLMs [2].

[1]: Siddiq, M. L., da Silva Santos, J. C., Devareddy, S., & Muller, A. (2024, October). Sallm: Security assessment of generated code. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering Workshops (pp. 54-65).<br/>
[2]: Cappendijk, T., de Reus, P., & Oprescu, A. (2024). Generating Energy-efficient code with LLMs. arXiv preprint arXiv:2411.10599.

# Installation

Clone the repository and install the required dependencies using pip:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate
pip install -r requirements.txt
```

If you want to run the code evaluation phase, you also need to setup a second virtual environment:

```bash
# Deactivate the current virtual environment if it is active
deactivate

cd ./evaluation
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate
pip install -r evaluation_requirements.txt
```

Note that the `evaluation_requirements.txt` file contains the expected dependencies that LLMs might have used in their generated code. For the already available generated code files, all used dependencies are incorporated in the `evaluation_requirements.txt` file. If other generated code files are added, the `evaluation_requirements.txt` file should be updated accordingly.

# Setup

SECUREGY is designed to be modular, allowing you to run different phases of the assessment framework independently. The framework consists of three main phases:

-   **Code Generation Phase**: This phase generates code using the configurations set by `experiment_configuration.py`.
-   **Code Evaluation Phase**: This phase evaluates the generated code using test-based testing and static code analysis. It requires a separate virtual environment to run the test-based testing.
-   **Result Parsing Phase**: This phase parses the results from the code evaluation phase and creates graphical representations of the findings that are stored in `/result_parsing/results` directory.

In `main.py`, you can specify which phases you want to run.

## Dataset

The dataset defines the prompts and unittests. The prompts are used to generate code and the need for unittests stems from the requirement to obtain executable code for each prompt. The ability to execute the generated code enables the conduct of energy efficiency measurements. SECUREGY is by default configured to use the prompts and unittests from the [SALLM dataset](https://github.com/s2e-lab/SALLM).

## Configuration

The configuration for the code generation phase is set in `experiment_configuration.py`. This file contains the settings for the LLMs, prompt engineering techniques, and other parameters that influence the code generation process. The file contains pydantic models that define the structure of the configuration. You can modify these models to add or remove parameters as needed.

SECUREGY is by default configured to use Ollama as the LLM provider. If you want to use a different LLM provider, you need to add the corresponding backend handler to `generation/backend_handlers.py`. The backend handlers are responsible for communicating with the LLMs and extracting the generated code based. The python file contains an abstract class `LLMBackendHandler` that defines the interface for the backend handlers. You can create a new class that inherits from this abstract class and implements the required methods.

# Output

From the result parsing phase, four graphs are created and stored in the `/result_parsing/results` directory:

1. **energy_efficiency_heatmap_between_bases.png**: A heatmap that shows the improvement in energy efficiency between different combinations of platform, model and temperature with the base PET.
2. **energy_efficiency_heatmap_to_base.png**: A heatmap that shows the improvement in energy efficiency between the use of different PETs and the base PET for each combination of platform, model and temperature.
3. **security_heatmap_between_bases.png**: A heatmap that shows the improvement in security between different combinations of platform, model and temperature with the base PET.
4. **security_heatmap_to_base.png**: A heatmap that shows the improvement in security between the use of different PETs and the base PET for each combination of platform, model and temperature.

# Testing

To run the tests, you can use the following command:

```bash
# Activate the virtual environment. Skip this step if you are already in the virtual environment.
source .venv/bin/activate # On Windows use: .venv\Scripts\activate
python -m pytest __tests__
```
